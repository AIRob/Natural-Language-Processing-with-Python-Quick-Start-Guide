{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning\n",
    "\n",
    "This is a code walkthrough for self-starters on most common text cleaning tasks.\n",
    "After the end of this notebook, you will be able to: \n",
    "1. Understand tokenization, do it manually yourself - and using spaCy\n",
    "2. Understand why stop word removal and case standardization work - with spaCy examples\n",
    "3. Differentiate between Stemming and Lemmatization - with spaCy lemmatization examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have always liked **The Adventures of Sherlock Holmes** by _Arthur Conan Doyle_. Let's download the book and save it locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://www.gutenberg.org/ebooks/1661.txt.utf-8'\n",
    "file_name = 'sherlock.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "# Download the file from `url` and save it locally under `file_name`:\n",
    "\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    with open(file_name, 'wb') as out_file:\n",
    "        data = response.read() # a `bytes` object\n",
    "        out_file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sherlock.txt\n"
     ]
    }
   ],
   "source": [
    "!ls {*.txt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!head -2 sherlock.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file contains header and footer information from Project Gutenberg. We are not interested in the same and will discard the copyright and other legal notices. \n",
    "Todo: \n",
    "- Open the file and delete the header and footer information and save the file as ```sherlock_clean.txt```\n",
    "\n",
    "I opened the text file to see that I need to remove the first 33 lines. Let's do that using shell commands - which also work on Windows inside Jupyter notebook: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sed -i 1,33d sherlock.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the ```sed``` syntax. \n",
    "\n",
    "The ```-i``` flag tells to make the changes in place.  \n",
    "```1,33d``` instructs to delete lines 1 to 33."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE ADVENTURES OF SHERLOCK HOLMES\n",
      "\n",
      "by\n",
      "\n",
      "SIR ARTHUR CONAN DOYLE\n"
     ]
    }
   ],
   "source": [
    "!head -5 sherlock.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do I see? \n",
    "\n",
    "Before I continue to text cleaning for any Natural Language Processing Task, I like to spend a few seconds taking a quick glance at the data itself. I noted down some of the things I spotted below, of course a trained eye can see a lot more than I did: \n",
    "\n",
    "1. Dates are written in a mixed format: `twentieth of March, 1888`, times are too: `three o'clock`\n",
    "1. Text is wrapped at around 70 columns, or no line can be longer than 70 characters \n",
    "1. There are lot of proper nouns. These include names such as `Atkinson`, `Trepoff` in addition to locations such as `Trinconmalee` and `Baker Street` etc.\n",
    "1. The index is in Roman numerals such as `I` and `IV` and not `1` or `4`\n",
    "1. There are lot of dialogues such as: \"You have carte blanche.\" with no narrative around them. This storytelling style switches freely from a narrative to a dialogue driven. \n",
    "1. The grammar and vocabulary is slightly unusual because of the time when Doyle wrote.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE A\n"
     ]
    }
   ],
   "source": [
    "#let's the load data to RAM\n",
    "text = open(file_name, 'r', encoding='utf-8').read()  # note that I add an encoding='utf-8' parameter to preserve information\n",
    "print(text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file is loaded as datatype: <class 'str'> and has 581204 characters in it\n"
     ]
    }
   ],
   "source": [
    "print(f'The file is loaded as datatype: {type(text)} and has {len(text)} characters in it')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Loaded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'â', 'è', 'é']\n",
      "There are 85 unique characters, including both ASCII and Unicode character\n"
     ]
    }
   ],
   "source": [
    "# how many unique characters do we see? \n",
    "# For reference, ASCII has 127 characters in it - so we expect this to have at most 127 characters\n",
    "unique_chars = list(set(text))\n",
    "unique_chars.sort()\n",
    "print(unique_chars)\n",
    "print(f'There are {len(unique_chars)} unique characters, including both ASCII and Unicode character')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our machine learning models, we often need the words to occur as individual tokens or single words. This process is called:\n",
    "\n",
    "## Tokenization \n",
    "\n",
    "We convert the raw text into a list of words. This preserves the original ordering of the text. \n",
    "\n",
    "### Split by Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107431\n"
     ]
    }
   ],
   "source": [
    "words = text.split()\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', 'THE', 'woman.', 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name.', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole', 'of', 'her', 'sex.', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', 'akin', 'to', 'love', 'for', 'Irene', 'Adler.', 'All', 'emotions,', 'and', 'that', 'one', 'particularly,', 'were', 'abhorrent', 'to', 'his', 'cold,', 'precise', 'but', 'admirably', 'balanced', 'mind.', 'He', 'was,', 'I', 'take', 'it,', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', 'the', 'world', 'has', 'seen,', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', 'false', 'position.', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions,', 'save', 'with', 'a', 'gibe', 'and', 'a', 'sneer.', 'They', 'were', 'admirable', 'things', 'for']\n"
     ]
    }
   ],
   "source": [
    "print(words[90:200])  #start with the first chapeter, ignoring the index for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red-headed', 'woman', 'on', 'the', 'street']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at another example: \n",
    "'red-headed woman on the street'.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the words red-headed were not split. This is something we may or may not want to keep always.  \n",
    "\n",
    "*Problem:* Punctuations are often appearing with the word itself, like: `Adler.` and `emotions,`.\n",
    "\n",
    "*Solution:* Simply extract words and discard everything else. This means we will discard all non-ASCII characters and punctuations.\n",
    "\n",
    "### Split by Word Extraction\n",
    "**Introducing Regex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Words', 'words', 'words', '']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.split('\\W+', 'Words, words, words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions can be daunting at first, but are very powerful. The regular expression `\\W+` means *a word character (A-Z etc.) repeated one or more times*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_alphanumeric = re.split('\\W+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109111, 107431)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_alphanumeric), len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BOHEMIA', 'I', 'To', 'Sherlock', 'Holmes', 'she', 'is', 'always', 'THE', 'woman', 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole', 'of', 'her', 'sex', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', 'akin', 'to', 'love', 'for', 'Irene', 'Adler', 'All', 'emotions', 'and', 'that', 'one', 'particularly', 'were', 'abhorrent', 'to', 'his', 'cold', 'precise', 'but', 'admirably', 'balanced', 'mind', 'He', 'was', 'I', 'take', 'it', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', 'the', 'world', 'has', 'seen', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', 'false', 'position', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions', 'save', 'with', 'a', 'gibe', 'and', 'a', 'sneer', 'They', 'were', 'admirable']\n"
     ]
    }
   ],
   "source": [
    "print(words_alphanumeric[90:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice how `Adler` no longer has the punctuation with her. This is what we wanted. Mission Accomplished.  \n",
    "\n",
    "**What was the tradeoff we made here?** To understand that, let's look at another example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Isn', 't', 'he', 'coming', 'home', 'for', 'dinner', 'with', 'the', 'red', 'headed', 'girl', '']\n"
     ]
    }
   ],
   "source": [
    "words_break = re.split('\\W+', \"Isn't he coming home for dinner with the red-headed girl?\")\n",
    "print(words_break)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have split `Isn't` to `Isn` and `t`. This is not good if you were working with say email or Twitter data, because you would've a lot more of such contractions. As a minor annoyance, we have an extra empty token at the end. \n",
    "\n",
    "Similarly, because we neglected punctuation `red-headed` is broken into two words: `red` and `headed`\n",
    "\n",
    "We can write custom rules in our tokenization strategy to cover all these cases. Or, use something which already has been written for us. \n",
    "\n",
    "### spaCy for Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above syntax creates a spaCy object `doc`. The object pre-computes a lot of linguistic features, including tokens. \n",
    "\n",
    "We can retrieve them by calling the object iterator. Below, we call the iterator and `list` it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[whole, of, her, sex, ., It, was, not, that, he, felt, \n",
      ", any, emotion, akin, to, love, for, Irene, Adler, ., All, emotions, ,, and, that, \n",
      ", one, particularly, ,, were, abhorrent, to, his, cold, ,, precise, but, \n",
      ", admirably, balanced, mind, ., He, was, ,, I, take, it, ,]\n"
     ]
    }
   ],
   "source": [
    "print(list(doc)[150:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conveniently, spaCy tokenizes all *punctuations and words* and returned those as individual tokens as well. Let's try the example which we didn't like earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Is, n't, he, coming, home, for, dinner, with, the, red, -, headed, girl, ?]\n"
     ]
    }
   ],
   "source": [
    "words = nlp(\"Isn't he coming home for dinner with the red-headed girl?\")\n",
    "print([token for token in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations*:\n",
    "- spaCy got the `Isn't` split as we wanted \n",
    "- `red-headed` was broken into 3 tokens: `red`, `-`, `headed`. Since the punctuation information isn't lost, we can restore the original `red-headed` token if we want to\n",
    "\n",
    "**How does the spaCy tokenizer work ?**\n",
    "\n",
    "> First, the raw text is split on whitespace characters, similar to text.split(' '). Then, the tokenizer processes the text from left to right. On each substring, it performs two checks:\n",
    "> \n",
    "> - **Does the substring match a tokenizer exception rule?** For example, \"don't\" does not contain whitespace, but should be split into two tokens, \"do\" and \"n't\", while \"U.K.\" should always remain one token.\n",
    "> - **Can a prefix, suffix or infix be split off?** For example punctuation like commas, periods, hyphens or quotes.\n",
    ">\n",
    "> ![caption](https://spacy.io/assets/img/tokenization.svg)\n",
    "> from [spaCy-101](https://spacy.io/usage/spacy-101) docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentence Tokenization**: We can also use spaCy to extract one sentence at a time, instead of one-word-at-a-time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I. A SCANDAL IN BOHEMIA\n",
      "\n",
      "I.\n",
      "\n",
      "To Sherlock Holmes, she is always THE woman., I have seldom heard\n",
      "him mention her under any other name., In his eyes she eclipses\n",
      "and predominates the whole of her sex., It was not that he felt\n",
      "any emotion akin to love for Irene Adler.]\n"
     ]
    }
   ],
   "source": [
    "sentences = list(doc.sents)\n",
    "print(sentences[13:18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STOP WORD REMOVAL & CASE CHANGE\n",
    "\n",
    "These simple ideas are widespread and fairly effective for a lot of tasks. They are particularly useful in reducing the number of unique tokens in a document for your processing.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy has already marked each token as a stop word or not and stored it in `is_stop` attribute of each token. This makes it very handy for text cleaning. Let's take a quick look: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_example = \"the AI/AGI uprising cannot happen without the progress of NLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(the, True, False),\n",
       " (AI, False, False),\n",
       " (/, False, True),\n",
       " (AGI, True, False),\n",
       " (uprising, False, False),\n",
       " (can, True, False),\n",
       " (not, True, False),\n",
       " (happen, False, False),\n",
       " (without, True, False),\n",
       " (the, True, False),\n",
       " (progress, False, False),\n",
       " (of, True, False),\n",
       " (NLP, True, False)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token, token.is_stop, token.is_punct) for token in nlp(sentence_example)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE False False\n",
      "ADVENTURES False False\n",
      "OF False False\n",
      "SHERLOCK False False\n",
      "HOLMES False False\n"
     ]
    }
   ],
   "source": [
    "for token in doc[:5]:\n",
    "    print(token, token.is_stop, token.is_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, while `the` and `of` were marked as stop words, `THE` and `OF` were not. This is not a bug but by design. spaCy doesn't remove words which are probably important because of their CAPS or Title Case automatically. \n",
    "\n",
    "We can instead force this behaviour by converting our original text to lower case before we pass it to spaCy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lower = text.lower()  # native python function\n",
    "doc_lower = nlp(text_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the True\n",
      "adventures False\n",
      "of True\n",
      "sherlock False\n",
      "holmes False\n"
     ]
    }
   ],
   "source": [
    "for token in doc_lower[:5]:\n",
    "    print(token, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spaCy has a dictionary of 305 stop words'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "f'spaCy has a dictionary of {len(list(STOP_WORDS))} stop words'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also extend the STOP WORDS dictionary on the fly for your domain. For instance, if you were using this code to process text of a NLP book, we might want to add words like `NLP`, `Processing`, `AGI`, `Data` etc. to stop words list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "domain_stop_words = [\"NLP\", \"Processing\", \"AGI\"]\n",
    "for word in domain_stop_words:\n",
    "    STOP_WORDS.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(the, True, False),\n",
       " (AI, False, False),\n",
       " (/, False, True),\n",
       " (AGI, True, False),\n",
       " (uprising, False, False),\n",
       " (can, True, False),\n",
       " (not, True, False),\n",
       " (happen, False, False),\n",
       " (without, True, False),\n",
       " (the, True, False),\n",
       " (progress, False, False),\n",
       " (of, True, False),\n",
       " (NLP, True, False)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token, token.is_stop, token.is_punct) for token in nlp(sentence_example)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly as expected, `NLP` and `AGI` are now marked as stop words too. \n",
    "\n",
    "Let's pull out string tokens which are not stop words into a Python list or similar data structure. \n",
    "Most NLP tasks which come after text pre-processing expect string tokens and not spaCy token objects as datatype. \n",
    "\n",
    "We will remove both stop words and punctuations here for demonstration: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI', 'uprising', 'happen', 'progress']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(token) for token in nlp(sentence_example) if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI', '/', 'uprising', 'happen', 'progress']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(token) for token in nlp(sentence_example) if not token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "> **Stemming** usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. \n",
    "\n",
    "> **Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma. \n",
    "\n",
    "> If confronted with the token `saw`, stemming might return just `s`, whereas lemmatization would attempt to return either `see` or `saw` depending on whether the use of the token was as a verb or a noun.\n",
    "\n",
    "> - Christopher Manning et al, 2008, [IR-Book](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy for Lemmatization\n",
    "\n",
    "**spaCy only supports lemmatization**. As discussed by spaCy core contributor in [issue #327](https://github.com/explosion/spaCy/issues/327) on Github, stemmer's are rarely a good idea. \n",
    "\n",
    "We want to treat `meet/NOUN` as different from `meeting/VERB`. Unlike Stanford NLTK which was created to introduce as many NLP ideas as possible, spaCy takes an opinionated stand against stemming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An underscore at end, such as `lemma_` tells spaCy we are looking for something which is human readable. spaCy stores the internal hash or identifier which spaCy stores in `token.lemma`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Their, '-PRON-', 561228191312463089, 'ADJ'),\n",
       " (Apples, 'apples', 14374618037326464786, 'PROPN'),\n",
       " (&, '&', 15473034735919704609, 'CCONJ'),\n",
       " (Banana, 'banana', 2525716904149915114, 'PROPN'),\n",
       " (fruit, 'fruit', 17674554054627885835, 'NOUN'),\n",
       " (salads, 'salad', 16382906660984395826, 'NOUN'),\n",
       " (are, 'be', 10382539506755952630, 'VERB'),\n",
       " (amazing, 'amazing', 12968186374132960503, 'ADJ'),\n",
       " (., '.', 12646065887601541794, 'PUNCT'),\n",
       " (Would, 'would', 6992604926141104606, 'VERB'),\n",
       " (you, '-PRON-', 561228191312463089, 'PRON'),\n",
       " (like, 'like', 18194338103975822726, 'VERB'),\n",
       " (meeting, 'meet', 6880656908171229526, 'VERB'),\n",
       " (me, '-PRON-', 561228191312463089, 'PRON'),\n",
       " (at, 'at', 11667289587015813222, 'ADP'),\n",
       " (the, 'the', 7425985699627899538, 'DET'),\n",
       " (cafe, 'cafe', 10569699879655997926, 'NOUN'),\n",
       " (?, '?', 8205403955989537350, 'PUNCT')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_sentence_example = \"Their Apples & Banana fruit salads are amazing. Would you like meeting me at the cafe?\"\n",
    "[(token, token.lemma_, token.lemma, token.pos_ ) for token in nlp(lemma_sentence_example)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a quite a few things going on here. Let's discuss them: \n",
    "\n",
    "**-PRON-**\n",
    "\n",
    "spaCy has a slightly annoying lemma (recall: lemma is the output of lemmatization): -PRON-. This is used as the lemma for all PRONouns such as `Their`, `you`, `me` and `I`. Some other NLP tools instead lemmatize to `I` instead of a placeholder `-PRON-`\n",
    "\n",
    "**(automatic) Lower casing**\n",
    "\n",
    "While checking for stop words, spaCy did not automatically lower case our input while comparison. On the other hand, while lemmatization it did. It converted `Apples` to `apple` and `Banana` to `banana`. \n",
    "\n",
    "**meeting to meet**\n",
    "\n",
    "Lemmatization is aware of the linguistic role the word plays in context. `Meeting` is converted to `meet` because it's a verb. spaCy does expose part of speech tagging and other linguistic features for us to use. We will learn how to query those soon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy comparison with Stanford CoreNLP and NLTK\n",
    "\n",
    "|Feature|\tSpacy|\tNLTK|\tCore NLP|\n",
    "        |---|---|-----|---|\n",
    "|Python API|\tY|\tY|\tN|\n",
    "|Multi Language support|\tY|\tY|\tY|\n",
    "|Tokenization|\tY|\tY|\tY|\n",
    "|Lemmatization| Y|\tY|\tY|\n",
    "|Stemming|\tN|\tY|\tY|\n",
    "|Part-of-speech tagging|\tY|\tY|\tY|\n",
    "|Sentence segmentation|\tY|\tY|\tY|\n",
    "|Dependency parsing|\tY|\tN|\tY|\n",
    "|Entity Recognition|\tY|\tY|\tY|\n",
    "|*Integrated word vectors*|\tY|\tN|\tN|\n",
    "|Sentiment analysis|\tY|\tY|\tY|\n",
    "|*Coreference resolution*|\tN|\tN|\tY|\n",
    "|*Built-in Text Classification*|Y | N| N|\n",
    "\n",
    "- corrected the partially wrong and outdated version on [AnalyticsVidhya](https://www.analyticsvidhya.com/blog/2017/04/natural-language-processing-made-easy-using-spacy-%E2%80%8Bin-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastAI",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
