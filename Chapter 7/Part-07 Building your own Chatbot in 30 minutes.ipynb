{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your own Chatbot\n",
    "\n",
    "Chatbots, or generally referred to conversation software are amazing tools for a lot of businesses. They help businesses server their clients server 24X7 without increasing effort, with consistent quality and the inbuilt option to defer to a human when the bots are not enough. \n",
    "\n",
    "They are great example where technology or AI has come to improve the impact human effort. \n",
    "\n",
    "They range from voice based solutions like Alexa to text based Intercom chat boxes to menu based navigation in Uber\n",
    "\n",
    "So far, we have seen one off application of every NLP topic that we have seen: \n",
    "- text cleaning using grammar and vocabulary insights \n",
    "- linguistics (and statistical parsers) to mine questions from text\n",
    "- entity recognition for information extraction\n",
    "- text similarity using text based vectors like GloVe/word2vec\n",
    "\n",
    "We now combine all of them into a lot more complicated setup and write our own chatbot, from scratch. But before you build anything from scratch, you should ask why.\n",
    "\n",
    "## Why should I build the service again? \n",
    "\n",
    "\n",
    "##### Related: Why can't I use FB/MSFT/some other cloud service?\n",
    "\n",
    "- **Privacy and Competition**: As a business, is it a good idea to share information of your users with a Facebook, or Microsoft? Or even a smaller company? \n",
    "- **Cost and Constraints**: Your funky cloud limits your design choices made by a particular intelligence provider to those made like Google or Facebook. Additionally, you are now paying for each http call you make, which is slower than locally running code too\n",
    "- **Freedom to Customize and Extend**: You can develop a solution which performs better for you! You don't have to cure world hunger, just keep shipping an ever increasing more business value via quality software. If you are a bigco, you have all the more reason to invest in extendible software\n",
    "\n",
    "For the sake of simplicity, we will assume that our bot does not need to remember the context of any question. So it sees one input, and responds to it and is done. No links established with previous input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors + Heuristic - Fancy Stuff = Quick Working Code\n",
    "\n",
    "Let's start by simply loading the word vectors using gensim as we have seen earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim version: 3.4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "print(f\"Gensim version: {gensim.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, please remove if you wish to download again\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "class TqdmUpTo(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None: self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "def get_data(url, filename):\n",
    "    \"\"\"\n",
    "    Download data if the filename does not exist already\n",
    "    Uses Tqdm to show download progress\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from urllib.request import urlretrieve\n",
    "    \n",
    "    if not os.path.exists(filename):\n",
    "\n",
    "        dirname = os.path.dirname(filename)\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "\n",
    "        with TqdmUpTo(unit='B', unit_scale=True, miniters=1, desc=url.split('/')[-1]) as t:\n",
    "            urlretrieve(url, filename, reporthook=t.update_to)\n",
    "    else:\n",
    "        print(\"File already exists, please remove if you wish to download again\")\n",
    "\n",
    "embedding_url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "get_data(embedding_url, 'data/glove.6B.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip data/glove.6B.zip \n",
    "# !mv -v glove.6B.300d.txt data/glove.6B.300d.txt \n",
    "# !mv -v glove.6B.200d.txt data/glove.6B.200d.txt \n",
    "# !mv -v glove.6B.100d.txt data/glove.6B.100d.txt \n",
    "# !mv -v glove.6B.50d.txt data/glove.6B.50d.txt \n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'data/glove.6B.300d.txt'\n",
    "word2vec_output_file = 'data/glove.6B.300d.txt.word2vec'\n",
    "import os\n",
    "if not os.path.exists(word2vec_output_file):\n",
    "    glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 49s, sys: 2.11 s, total: 1min 51s\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import KeyedVectors\n",
    "filename = word2vec_output_file\n",
    "embed = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check if we can vectorize any word by checking for word embedding for any word e.g. 'awesome' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert embed['awesome'] is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'awesome', this works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the first challenge for us: **Figuring out the right user intent**.\n",
    "\n",
    "This is commonly referred to as the problem of intent categorization. \n",
    "\n",
    "As a toy example, we will try to build an order bot which someone like DoorDash/Swiggy/Zomato might use. \n",
    "\n",
    "## Use Case: Food Order Bot\n",
    "\n",
    "Consider the following sample sentence:  “I’m looking for a cheap Chinese place in Indiranagar”\n",
    "\n",
    "We want to pick out Chinese as a cuisine type in the sentence. We can obviously do simple approaches, like exact substring match (search \"Chinese\") or TF-IDF based matches. \n",
    "\n",
    "Instead, we will generalize the model to discover cuisine types which we might not have identified yet, but we can learn about them via the GloVe embedding:\n",
    "\n",
    "We’ll keep it as simple as possible: \n",
    "- provide some example cuisine types to tell the model that we need cuisines, and \n",
    "- look for the most similar words in the sentence\n",
    "\n",
    "We’ll loop through the words in the sentence, and pick out the ones whose average cosine similarity to the reference words is above some threshold.\n",
    "\n",
    "### Do word vectors even work for this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisine_refs = [\"mexican\", \"thai\", \"british\", \"american\", \"italian\"]\n",
    "sample_sentence = \"I’m looking for a cheap Indian or Chinese place in Indiranagar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity's sake, the code below is written as for loops, but can be vectorized for speed. \n",
    "\n",
    "We iterate over each word in the input sentence and find the similarity score with respect to known cuisine words. \n",
    "\n",
    "The higher the value, the more likely is the word to be something related to our cuisine references or `cuisine_refs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking: 7.448504447937012\n",
      "for: 10.627421379089355\n",
      "a: 11.809560775756836\n",
      "cheap: 7.09670877456665\n",
      "indian: 18.64516258239746\n",
      "or: 9.692893981933594\n",
      "chinese: 19.09498405456543\n",
      "place: 7.651237487792969\n",
      "in: 10.085711479187012\n",
      "['indian', 'chinese']\n"
     ]
    }
   ],
   "source": [
    "tokens = sample_sentence.split()\n",
    "tokens = [x.lower().strip() for x in tokens] \n",
    "threshold = 18.3\n",
    "found = []\n",
    "for term in tokens:\n",
    "    if term in embed.vocab:\n",
    "        scores = []\n",
    "        for C in cuisine_refs:\n",
    "            scores.append(np.dot(embed[C], embed[term].T))\n",
    "            # hint replace above above np.dot with: \n",
    "            # scores.append(embed.cosine_similarities(<vector1>, <vector_all_others>))\n",
    "        mean_score = np.mean(scores)\n",
    "        print(f\"{term}: {mean_score}\")\n",
    "        if mean_score > threshold:\n",
    "            found.append(term)\n",
    "print(found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The threshold is determined empirically. Notice that we are able to infer 'Indian' and 'Chinese' as cuisines, even if they are not part of the original set. \n",
    "\n",
    "Ofcourse, exact matches will have a much higher score.\n",
    "\n",
    "This is a good example where a better problem formulation, in terms of \"generic\" cuisine type which can be learned is more helpful than a dictionary based cuisine type. This also proves that we can rely on word vector based approaches. \n",
    "\n",
    "Can we extend this for user intent classification? Let's try this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Stop: Classifying user intent\n",
    "\n",
    "We want to be able to put sentences in categories by user \"intents\". Intents are a generic mechanism which combine multiple individual example into one semantic umbrella e.g. \"hi\", \"hey\", \"good morning\", \"wassup!\" are all example of the intent: _greeting_\n",
    "\n",
    "Using 'greeting' as an input, the back end logic can then determin how to respond to user. \n",
    "\n",
    "There are many ways we could combine word vectors to represent a sentence, but again we’re going to do the simplest thing possible: add them up. \n",
    "\n",
    "This is definitely a less than ideal solution, but works in practice because of the simple unsupervised approach we use with this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "def sum_vecs(embed,text):\n",
    "\n",
    "    tokens = text.split(' ')\n",
    "    vec = np.zeros(embed.vector_size)\n",
    "\n",
    "    for idx, term in enumerate(tokens):\n",
    "        if term in embed.vocab:\n",
    "            vec = vec + embed[term]\n",
    "    return vec\n",
    "\n",
    "sentence_vector = sum_vecs(embed, sample_sentence)\n",
    "print(sentence_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a data dictionary with some examples for each intent. This dictionary can be updated as we have more user inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={\n",
    "  \"greet\": {\n",
    "    \"examples\" : [\"hello\",\"hey there\",\"howdy\",\"hello\",\"hi\",\"hey\",\"hey ho\"],\n",
    "    \"centroid\" : None\n",
    "  },\n",
    "  \"inform\": {\n",
    "    \"examples\" : [\n",
    "        \"i'd like something asian\",\n",
    "        \"maybe korean\",\n",
    "        \"what mexican options do i have\",\n",
    "        \"what italian options do i have\",\n",
    "        \"i want korean food\",\n",
    "        \"i want german food\",\n",
    "        \"i want vegetarian food\",\n",
    "        \"i would like chinese food\",\n",
    "        \"i would like indian food\",\n",
    "        \"what japanese options do i have\",\n",
    "        \"korean please\",\n",
    "        \"what about indian\",\n",
    "        \"i want some chicken\",\n",
    "        \"maybe thai\",\n",
    "        \"i'd like something vegetarian\",\n",
    "        \"show me french restaurants\",\n",
    "        \"show me a cool malaysian spot\",\n",
    "        \"where can I get some spicy food\"\n",
    "    ],\n",
    "    \"centroid\" : None\n",
    "  },\n",
    "  \"deny\": {\n",
    "    \"examples\" : [\n",
    "      \"nah\",\n",
    "      \"any other places ?\",\n",
    "      \"anything else\",\n",
    "      \"no thanks\"\n",
    "      \"not that one\",\n",
    "      \"i do not like that place\",\n",
    "      \"something else please\",\n",
    "      \"no please show other options\"\n",
    "    ],\n",
    "    \"centroid\" : None\n",
    "  },\n",
    "    \"affirm\":{\n",
    "        \"examples\":[\n",
    "            \"yeah\",\n",
    "            \"that works\",\n",
    "            \"good, thanks\",\n",
    "            \"this works\",\n",
    "            \"sounds good\",\n",
    "            \"thanks, this is perfect\",\n",
    "            \"just what I wanted\"\n",
    "        ],\n",
    "        \"centroid\": None\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach we have is simple, we find the centroid of each \"user intent\". A centroid is just a central point to denote each intent. Then, the incoming text is assigned to the user intent nearest to the corresponding cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroid(embed, examples):\n",
    "    C = np.zeros((len(examples),embed.vector_size))\n",
    "    for idx, text in enumerate(examples):\n",
    "        C[idx,:] = sum_vecs(embed,text)\n",
    "\n",
    "    centroid = np.mean(C,axis=0)\n",
    "    assert centroid.shape[0] == embed.vector_size\n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Centroid to data dictionary\n",
    "for label in data.keys():\n",
    "    data[label][\"centroid\"] = get_centroid(embed,data[label][\"examples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greet: ['hello', 'hey there', 'howdy', 'hello', 'hi', 'hey', 'hey ho']\n",
      "inform: [\"i'd like something asian\", 'maybe korean', 'what mexican options do i have', 'what italian options do i have', 'i want korean food', 'i want german food', 'i want vegetarian food', 'i would like chinese food', 'i would like indian food', 'what japanese options do i have', 'korean please', 'what about indian', 'i want some chicken', 'maybe thai', \"i'd like something vegetarian\", 'show me french restaurants', 'show me a cool malaysian spot', 'where can I get some spicy food']\n",
      "deny: ['nah', 'any other places ?', 'anything else', 'no thanksnot that one', 'i do not like that place', 'something else please', 'no please show other options']\n",
      "affirm: ['yeah', 'that works', 'good, thanks', 'this works', 'sounds good', 'thanks, this is perfect', 'just what I wanted']\n"
     ]
    }
   ],
   "source": [
    "for label in data.keys():\n",
    "    print(f\"{label}: {data[label]['examples']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intent(embed,data, text):\n",
    "    intents = list(data.keys())\n",
    "    vec = sum_vecs(embed,text)\n",
    "    scores = np.array([ np.linalg.norm(vec-data[label][\"centroid\"]) for label in intents])\n",
    "    return intents[np.argmin(scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text : 'hey ', predicted_label : 'greet'\n",
      "text : 'i am looking for chinese food', predicted_label : 'inform'\n",
      "text : 'not for me', predicted_label : 'deny'\n",
      "text : 'ok, this is good', predicted_label : 'affirm'\n"
     ]
    }
   ],
   "source": [
    "for text in [\"hey \",\"i am looking for chinese food\",\"not for me\", \"ok, this is good\"]:\n",
    "    print(f\"text : '{text}', predicted_label : '{get_intent(embed, data, text)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bot Responses\n",
    "\n",
    "We now know how to understand and categorize user intent. We now need to simply respond to each user intent with some corresponding responses. Let's these 'template' bot responses in one place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {\n",
    "        \"utter_greet\": [\"hey there!\", \"Hey! How you doin'? \"],\n",
    "        \"utter_options\": [\"ok, let me check some more\"],\n",
    "        \"utter_goodbye\": [\"Great, I'll go now. Bye bye\", \"bye bye\", \"Goodbye!\"],\n",
    "        \"utter_default\": [\"Sorry, I didn't quite follow\"],\n",
    "        \"utter_confirm\": [\"Got it\", \"Gotcha\", \"Your order is confirmed now\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the Response map in a separate entity is helpful. This means you can generate responses at a separate service from your intent understanding module and then glue them together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_map = {\n",
    "    \"greet\": \"utter_greet\",\n",
    "    \"affirm\": \"utter_goodbye\",\n",
    "    \"deny\": \"utter_options\",\n",
    "    \"inform\": \"utter_confirm\",\n",
    "    \"default\": \"utter_default\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'll notice carefully, there is no need for response map to be depend only on the intent categorized. You can convert this response map to a separate function which generates the map using related context and then picks a bot template. \n",
    "\n",
    "But here, for simplicity, let's keep it as a human interpretable dictionary/JSON style structure. \n",
    "\n",
    "Let's write a simple get_bot_response function which takes in the response mapping, templates, and the intent as inputs and return the actual bot response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_bot_response(bot_response_map, bot_templates, intent):\n",
    "    if intent not in list(response_map):\n",
    "        intent = \"default\"\n",
    "    select_template = bot_response_map[intent]\n",
    "    templates = bot_templates[select_template]\n",
    "    return random.choice(templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly tests this with one sentence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Got it'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_intent = get_intent(embed, data, \"i want indian food\")\n",
    "get_bot_response(response_map, templates, user_intent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Better Response Personalisation?**:\n",
    "\n",
    "You'll notice that the function picks one template at random for any particular \"bot intent\" so to say. While this is for simplicity here, in practice - you can train a ML model to pick a response personalized to user. \n",
    "\n",
    "A simple personalization to make is to be more coherent with the talking/typing style of the user itself. E.g. some user might be formal with \"Hello, how are you today?\", while other might be more informal with \"yo\". \n",
    "\n",
    "So \"Hello\" gets \"Goodbye!\" in response while \"yo!\" gets \"bye bye\" or even \"ttyl\" in the same conversation. \n",
    "\n",
    "---\n",
    "\n",
    "For now, let's go ahead and check the bot response for the sentences which we have already seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text : 'hey', intent: greet, bot: Hey! How you doin'? \n",
      "text : 'i am looking for italian food', intent: inform, bot: Gotcha\n",
      "text : 'not for me', intent: deny, bot: ok, let me check some more\n",
      "text : 'ok, this is good', intent: affirm, bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "for text in [\"hey\",\"i am looking for italian food\",\"not for me\", \"ok, this is good\"]:\n",
    "    user_intent = get_intent(embed, data, text)\n",
    "    bot_reply = get_bot_response(response_map, templates, user_intent)\n",
    "    print(f\"text : '{text}', intent: {user_intent}, bot: {bot_reply}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
